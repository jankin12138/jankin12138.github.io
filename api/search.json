[{"id":"e3447cde20e84ead44a473b80d153b14","title":"hexo+aurora+github+gitalk搭建属于自己的个人博客","content":"\n\n\n\n\n\n\n注意\n本文前八章来自于: 叁鄕浪子原作者写于2022年7月15日本人在基础上有所增加，并适当修改了章节顺序，配套的教学视频可在B站查看。\n\n\n1 node js的安装及环境配置\n\n\n\n\n\n\n\n\n直接访问Node.js的官方网站下载即可\n安装可以无脑下一步，注意安装路径就行，不会有问题，如果不放心的话可以参考原文章的安装过程截图，有详细步骤，由于本文篇幅较长，这里就不再转载了。\n测试成功方法如下：\n\n\n打开windows终端（按下win+R输入cmd）\n\nnode -v\n\nnpm -v\n\n如果安装成功会出现版本号码，如下图所示:\n1.2 配置环境变量以管理员身份-打开cmd，配置路径\nnpm config set prefix \"E:\\develop\\nodejs\\node_global\"\n\nnpm config set cache \"E:\\develop\\nodejs\\node_cache\"\n\n修改全局路径 node_global\n修改缓存路径 node_cache内路径需要根据自己实际情况来进行修改\n\n\n\n\n\n\n\n注意\n修改全局安装路径后，需要在系统环境变量Path中添加该路径，否则之后使用npm install –global xxx，xxx都报错找不到命令。\n\n\n找到电脑环境配置\n\nwin10 &amp; win11：右键此电脑-属性-高级系统设置-高级-环境变量\nwin11 还可以：点设置-系统-关于-高级系统设置-高级-环境变量\n\n2 安装git2.1 下载git访问Git 找到首页下方的Downloads\n下载对应系统（MAC、Windows、Linux&#x2F;Unix）安装包\n\n\n\n\n\n\n\n特别注意\n想省事可以直接无脑 Next(下一步)，带 New(新)的新功能不要选就是了,如果想了解详细的安装过程可以参考文章顶部的原文。\n\n\n3 hexo 下载npm install hexo-cli -g #安装hexo\n\n\n\n\n\n\n\n\n\n\n中间如果出现各种报错，可以先不用管，重新运行上述代码，直至成功因为网络原因导致的报错占大部分（因为墙的问题，也可以适当的使用科技）\n4 检查安装\nnode:\n\nnode -v\n\n\n\nnpm:\n\nnpm -v\n\n\ngit:\n\ngit --version\n\n\nhexo:\n\nhexo -v\n\n\n安装成功后截图：\n\n\n5 创建仓库及配置SSH连接5.1 创建github仓库5.1参考视频教程\n利用github仓库，存放静态网站资源，达到挂载网站的目的。\n需要注意的是作为网站访问的这个仓库，仓库名称一定是，拥有者名+github.io\n5.2 生成ssh keys5.2参考视频教程\n在博客文件夹根目录下，右键，调用git bash here功能\n先输入ssh查看是否已经安装ssh，git默认有安装，如下图所示就是安装过了。\n\n本地生成ssh keys,注意这里的邮箱地址是你github的注册邮箱地址\nssh-keygen -t rsa -C \"邮箱地址\"\n\n\n\n在本地电脑中找到.ssh\n\n\n一般默认都是，C:\\Users\\用户名.ssh\\id_rsa.pub\n找到秘钥的位置，并用记事本打开，复制其内容 (ctrl+a全选，ctrl+c复制，ctrl+v粘贴)\n打开github，头像箭头，下拉选项setting（设置）-SSH与GPG keys -new ssh keys（新建ssh秘钥）,把在本地生成的秘钥内容粘贴至此秘钥处，标题可以随便取。\n\n为了后面流程，在github里顺便设置person access tokens（个人访问令牌）\n(与ssh选项同一列，下面选择Developer setting log -Generate new token)\n\n下面勾选权限，建议全部勾选\n点击生成，生成的序列号\n\n\n\n\n\n\n\n特别注意\n注意保存\n要保存（复制&#x2F;截图）下来在存在本地，他只显示一次，如果忘记了，还需要重新生成一次。\n\n\n测试ssh是否绑定成功（在git里操作）\nssh -T git@github.com\n\n\n6 搭建本地博客创建一个放置博客文件夹的文件，在里面启用git Bash here，这里也可以用vscode打开终端管理。\n\n\n初始化hexo\n\nhexo init\n\n\n\n生成hexo本地页面\n\nhexo s\n\n\n\n复制粘贴该地址到浏览器中，即可访问本地搭建的博客 http://localhost=4000\n\nhexo cl #clean #清理编译文件\nhexo g #generate #编译项目\nhexo s #server #本地预览运行项目\n\n7 上传至本地博客至GitHub7.1 修改配置文件在创建博客文件夹的根目录下修改-config.yml文件\n\ndeploy:\n  type: git\n  repository: 你的github地址（就是你github.io那个仓库的地址）\n  branch: main\n\n7.2 安装hexo-deployer-git 自动部署发布工具npm install hexo-deployer-git --save\n\n7.3 编译文件生成页面hexo g\n\n7.4 本地文件上传到Githubhexo d\n\n\n\n\n\n\n\n提示\n在上传时，浏览器会跳出关于github的验证,要耐心等候\n\n\n\n输入用户名\n\n输入令牌\n\n\n(就是在github生成的那个，切记要保存)\n\n成功后可以直接访问http:&#x2F;&#x2F;你的用户名+.github.io访问\n\n8 安装Aurora主题\n\n\n\n\n\n\n\n\nAurora官方文档参考：https://aurora.tridiamond.tech/zh/guide\n8.1 配置npm install hexo-theme-aurora --save #进入hexo初始化目录用git执行\n\n\n因为主题是使用 NPM 或者 Yarn 安装的，而不是 clone 到 themes 文件夹的。\n所以我们需要自己创建一个配置文件。你只需要在 Hexo 博客的根目录下创建一个_config.aurora.yml 配置文件来配置主题\n此时打开配置文件发现是空的我们可以到node_modules下找到hexo-theme-aurora（.\\node_modules\\hexo-theme-aurora_config.yml），将改文件内容复制到根目录下的_config.aurora.yml。\n\n8.2 修改配置打开_comfig.yml\n由默认主题改为Aurora\n\n\n由于Aurora是vue3项目\n打开根目录下的_config.yml\n修改路由方式\n\n运行\nhexo clean &amp; hexo g &amp; hexo server\n\n\n8.3 上传并覆盖GitHub仓库hexo d\n\n打开仓库地址 主题配置成功\n\n9 参考教程\n\n\n\n\n\n\n\n\n【2021最新版】保姆级Hexo+github搭建个人博客\nHexo博客+Aurora主题安装与配置_神秘布偶猫\nhexo+aurora+github搭建 | 致彩之镜 (sxlz.me)\nAurora主题安装\nhexo+github搭建个人博客\n博客搭建日志\nAurora主题官方文档\n","slug":"hexo-aurora-github搭建属于自己的个人博客","date":"2022-09-17T09:48:24.000Z","categories_index":"技术教程","tags_index":"hexo,博客搭建","author_index":"依水何安"},{"id":"76d4212e70fc90d13d8dd3025d415413","title":"事务隔离&数据库索引","content":"\n\n\n\n\n\n\n\n\n之前在字节青训营因为大作业的相关内容也接触过一些数据库方向的知识，但一直苦于没有系统性的学习和整理，所以这系列文章来总结和记录一些，个人觉得比较重要的内容，以便以后复习使用。主要参考文章是极客时间的MySQL45讲：MySQL 实战 45 讲 这次主要是事务和索引方面的内容，感觉写在一起太长了也不方便记录所以分成几个部分来写，如果后续觉得不方便查阅可能会合并。\n事务隔离\n\n\n\n\n\n\n\n\n简单来说，事务是由一条SQL语句，或者一组SQL语句组成的程序执行单元，事务就是要保证一组数据库操作，要么全部成功，要么全部失败。在 MySQL 中，事务支持是在引擎层实现的。\n\n支付举例\n最经典的例子就是转账，你要给朋友小王转 100 块钱，而此时你的银行卡只有 100 块钱。转账过程具体到程序里会有一系列的操作，比如查询余额、做加减法、更新余额等，这些操作必须保证是一体的，不然等程序查完之后，还没做减法之前，你这 100 块钱，完全可以借着这个时间差再查一次，然后再给另外一个朋友转账。因此事务的作用就是保证一组数据库操作保持一致。\n\n\n隔离性\n\n\n\n\n\n\n\n\nACDI事务四大特性\n1.原子性（Atomicity）：指事务内所有操作要么一起执行成功，要么都一起失败(或者说是回滚)；如事务经典转账案例：A给B转账，A把钱扣了，但B没有收到；可见这种错误是不能接受的，最终会回滚，这也是原子性的重要性。\n2.一致性（Consistency）：指事务执行前后的状态一致，如事务经典转账案例：A给B互相转账，不管怎么转，最终两者钱的总和还是不变。\n3.持久性（Durability）：事务一旦提交，数据就已经永久保存了，不能再回滚。\n4.隔离性（Isolation）：指多个并发事务之间的操作互不干扰，但是事务的并发可能会导致数据脏读、不可重复读、幻读问题，根据业务情况，采用事务隔离级别进行对应数据读问题处理。\n隔离级别\n\n\n\n\n\n\n\n\nSQL 标准定义了四种隔离级别，MySQL 全都支持。这4种隔离级别，并行性能依次降低，安全性依次提高：\n\n读未提交（READ UNCOMMITTED）一个事务还没提交时，它做的变更就能被别的事务看到.\n读已提交 （READ COMMITTED）一个事务提交之后，它做的变更才会被其他事务看到。\n可重复读 （REPEATABLE READ）一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。\n串行化 （SERIALIZABLE）顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。\n\n\n视图实现\n在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。\n\n在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。\n\n在“读提交”隔离级别下，这个视图是在每个 SQL 语句开始执行的时候创建的。\n\n“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；\n\n“串行化”隔离级别下直接用加锁的方式来避免并行访问。\n\n\n\n\n\n可重复读场景\n假设你在管理一个个人银行账户表。一个表存了每个月月底的余额，一个表存了账单明细。这时候你要做数据校对，也就是判断上个月的余额和当前余额的差额，是否与本月的账单明细一致。你一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响你的校对结果。\n这时候使用“可重复读”隔离级别就很方便。事务启动时的视图可以认为是静态的，不受其他事务更新的影响。\n\n\n事务隔离实现\n\n\n\n\n\n\n\n\n在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。当系统里没有比这个回滚日志更早的 read-view 的时候，系统会判断没有事务再需要用到这些回滚日志，此时回滚日志会被删除。\n\n\n\n\n\n\n提示\n尽量不要使用长业务\n\n\n长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。\n在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库。\n事物的启动方式MySQL 的事务启动方式有以下几种：\n\n显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。\nset autocommit&#x3D;0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。\n\n\n\n\n\n\n\n\n注意\n有些客户端连接框架会默认连接成功后先执行一个 set autocommit&#x3D;0 的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。\n因此，建议总是使用 set autocommit&#x3D;1, 通过显式语句的方式来启动事务。\n\n\n\n\n\n\n\n\nQuestion\n现在知道了系统里面应该避免长事务，如果你是业务开发负责人同时也是数据库负责人，你会有什么方案来避免出现或者处理这种情况呢？\n\n\n\nAnswer\n首先，从应用开发端来看：\n\n确认是否使用了 set autocommit&#x3D;0。这个确认工作可以在测试环境中开展，把 MySQL 的 general_log 开起来，然后随便跑一个业务逻辑，通过 general_log 的日志来确认。一般框架如果会设置这个值，也就会提供参数来控制行为，你的目标就是把它改成 1。\n确认是否有不必要的只读事务。有些框架会习惯不管什么语句先用 begin&#x2F;commit 框起来。我见过有些是业务并没有这个需要，但是也把好几个 select 语句放到了事务中。这种只读事务可以去掉。\n业务连接数据库的时候，根据业务本身的预估，通过 SET MAX_EXECUTION_TIME 命令，来控制每个语句执行的最长时间，避免单个语句意外执行太长时间。（为什么会意外？在后续的文章中会提到这类案例）\n\n其次，从数据库端来看：\n\n监控 information_schema.Innodb_trx 表，设置长事务阈值，超过就报警 &#x2F; 或者 kill；\nPercona 的 pt-kill 这个工具不错，推荐使用；\n在业务功能测试阶段要求输出所有的 general_log，分析日志行为提前发现问题；\n如果使用的是 MySQL 5.6 或者更新版本，把 innodb_undo_tablespaces 设置成 2（或更大的值）。如果真的出现大事务导致回滚段过大，这样设置后清理起来更方便。\n\n\n\n数据库索引\n\n\n\n\n\n\n\n\n一句话简单来说，索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。\n\n\n\n\n\n\n\n特别注意\n对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。\n\n\n常见模型1.哈希表（K-V）存储哈希表是一种以键 - 值（key-value）存储数据的结构，我们只要输入待查找的值即 key，就可以找到其对应的值即 Value。结构如下图所示：\n\n需要注意的是，图中哈希表的的值并不是递增的，这样做的好处是增加新的 User 时速度会很快，只需要往后追加。但缺点是，因为不是有序的，所以哈希索引做区间查询的速度是很慢的。所以，哈希表这种结构适用于只有等值查询的场景。\n2.有序数组有序数组在等值查询和范围查询场景中的性能就都非常优秀。因为每个数组都可以保证递增，所以可以使用二分法就可以快速得到，这个时间复杂度是 O(log(N))。\n如果仅仅看查询效率，有序数组就是最好的数据结构了。但是，在需要更新数据的时候就麻烦了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。所以，有序数组索引只适用于静态存储引擎\n3.二叉搜索树\n\n\n\n\n\n\n\n\n二叉搜索树，它或者是一棵空树，或者是具有下列性质的二叉树： 若它的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 若它的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 它的左、右子树也分别为二叉搜索树。\n\n这个树的查询时间复杂度当然是O(log(N))，当然为了维持 O(log(N)) 的查询复杂度，你就需要保持这棵树是平衡二叉树。为了做这个保证，更新的时间复杂度也是 O(log(N))。\n\n\n\n\n\n\n提示\n树可以有二叉，也可以有多叉。多叉树就是每个节点有多个儿子，儿子之间的大小保证从左到右递增。二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上。为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N 叉”树。这里，“N 叉”树中的“N”取决于数据块的大小。\n\n\n以 InnoDB 的一个整数字段索引为例，这个 N 差不多是 1200。这棵树高是 4 的时候，就可以存 1200 的 3 次方个值，这已经 17 亿了。考虑到树根的数据块总是在内存中的，一个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要访问 3 次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。\nN 叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了。\nInnoDB 的索引模型在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。又因为前面我们提到的，InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。\n每一个索引在 InnoDB 里面对应一棵 B+ 树。\n假设，我们有一个主键列为 ID 的表，表中有字段 k，并且在 k 上有索引。\n这个表的建表语句是：\nmysql&gt; create table T(\nid int primary key, \nk int not null, \nname varchar(16),\nindex (k))engine&#x3D;InnoDB;\n\n表中 R1~R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)，两棵树的示例示意图如下。\n\n\n\n\n从图中不难看出，根据叶子节点的内容，索引类型分为主键索引和非主键索引。\n\n主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）。\n\n非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）。\n\n\n基于主键索引和普通索引的查询有什么区别？\n\n如果语句是 select * from T where ID&#x3D;500，即主键查询方式，则只需要搜索 ID 这棵 B+ 树；\n如果语句是 select * from T where k&#x3D;5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。\n\n也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。\n索引维护插入新记录的时候可以不指定 ID 的值，系统会获取当前 ID 最大值加 1 作为下一条记录的 ID 值。\n也就是说，自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。\n而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。\n最左前缀原则B+ 树这种索引结构，可以利用索引的“最左前缀”，来定位记录。\n如果你要查的是所有名字第一个字是“张”的人，你的 SQL 语句的条件是”where name like ‘张 %’”。这时，你也能够用上这个索引，查找到第一个符合条件的记录是 ID3，然后向后遍历，直到不满足条件为止。\n可以看到，不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。\n那么，如果既有联合查询，又有基于 a、b 各自的查询呢？查询条件里面只有 b 的语句，是无法使用 (a,b) 这个联合索引的，这时候你不得不维护另外一个索引，也就是说你需要同时维护 (a,b)、(b) 这两个索引。\n\n\n\n\n\n\n提示\n这时候，我们要考虑的原则就是空间了。比如上面这个市民表的情况，name 字段是比 age 字段大的 ，那我就建议你创建一个（name,age) 的联合索引和一个 (age) 的单字段索引。\n\n\n索引下推我们还是以市民表的联合索引（name, age）为例。如果现在有一个需求：检索出表中“名字第一个字是张，而且年龄是 10 岁的所有男孩”。那么，SQL 语句是这么写的：\nmysql&gt; select * from tuser where name like &#39;张 %&#39; and age&#x3D;10 and ismale&#x3D;1;\n\n你已经知道了前缀索引规则，所以这个语句在搜索索引树的时候，只能用 “张”，找到第一个满足条件的记录 ID3。当然，这还不错，总比全表扫描要好。\n\n在 MySQL 5.6 之前，只能从 ID3 开始一个个回表。到主键索引上找出数据行，再对比字段值。\n\n而 MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。\n\n\n普通索引与唯一索引\n\n\n\n\n\n\n\n\n1、普通索引\n　　普通索引（由关键字KEY或INDEX定义的索引）的唯一任务是加快对数据的访问速度。因此，应该只为那些最经常出现在查询条件（WHEREcolumn&#x3D;）或排序条件（ORDERBYcolumn）中的数据列创建索引。只要有可能，就应该选择一个数据最整齐、最紧凑的数据列（如一个整数类型的数据列）来创建索引。\n2、唯一索引\n　　普通索引允许被索引的数据列包含重复的值。比如说，因为人有可能同名，所以同一个姓名在同一个“员工个人资料”数据表里可能出现两次或更多次。\nchange buffer当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。\n需要说明的是，虽然名字叫作 change buffer，实际上它是可以持久化的数据。也就是说，change buffer 在内存中有拷贝，也会被写入到磁盘上。\n将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。\n显然，如果能够将更新操作先记录在 change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。\n:::deatils 什么条件下可以使用 change buffer \n对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入 (4,400) 这个记录，就要先判断现在表中是否已经存在 k&#x3D;4 的记录，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用 change buffer 了。\n因此，唯一索引的更新就不能使用 change buffer，实际上也只有普通索引可以使用。\nchange buffer 用的是 buffer pool 里的内存，因此不能无限增大。change buffer 的大小，可以通过参数 innodb_change_buffer_max_size 来动态设置。这个参数设置为 50 的时候，表示 change buffer 的大小最多只能占用 buffer pool 的 50%。\n:::\n优化器逻辑\n\n\n\n\n\n\n\n\n优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库里面，扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的 CPU 资源越少。\n扫描行数是怎么判断的？\nMySQL 在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数。\n这个统计信息就是索引的“区分度”。显然，一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数，我们称之为“基数”（cardinality）。也就是说，这个基数越大，索引的区分度越好。\nMySQL 是怎样得到索引的基数的呢？\nMySQL 采样统计的方法。\n因为把整张表取出来一行行统计，虽然可以得到精确的结果，但是代价太高了，所以只能选择“采样统计”。\n采样统计的时候，InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。\n在 MySQL 中，有两种存储索引统计的方式，可以通过设置参数 innodb_stats_persistent 的值来选择：\n\n设置为 on 的时候，表示统计信息会持久化存储。这时，默认的 N 是 20，M 是 10。\n设置为 off 的时候，表示统计信息只存储在内存中。这时，默认的 N 是 8，M 是 16。\n\n索引选择异常和处理\n采用 force index 强行选择一个索引。\n\n修改语句，引导 MySQL 使用我们期望的索引。\n\n在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。\n\n\n前缀索引与覆盖索引\n\n\n\n\n\n\n\n\n使用前缀索引就用不上覆盖索引对查询性能的优化了\n但是使用前缀索引通常效果不错，如何提高前缀索引的区分度：\n\n第一种方式是使用倒序存储。由于身份证号的最后 6 位没有地址码这样的重复逻辑，所以最后这 6 位很可能就提供了足够的区分度。\n\n第二种方式是使用 hash 字段。然后每次插入新记录的时候，都同时用 crc32() 这个函数得到校验码填到这个新字段。\n\n\n这两种方法的异同点：\n\n\n\n\n\n\n相同点\n\n它们的相同点是，都不支持范围查询。\n\n倒序存储的字段上创建的索引是按照倒序字符串的方式排序的，已经没有办法利用索引方式查出身份证号码在 [ID_X, ID_Y] 的所有市民了。同样地，hash 字段的方式也只能支持等值查询。\n\n\n\n\n\n\n\n\n不同点\n\n从占用的额外空间来看，倒序存储方式在主键索引上，不会消耗额外的存储空间，而 hash 字段方法需要增加一个字段。当然，倒序存储方式使用 4 个字节的前缀长度应该是不够的，如果再长一点，这个消耗跟额外这个 hash 字段也差不多抵消了。\n\n在 CPU 消耗方面，倒序方式每次写和读的时候，都需要额外调用一次 reverse 函数，而 hash 字段的方式需要额外调用一次 crc32() 函数。如果只从这两个函数的计算复杂度来看的话，reverse 函数额外消耗的 CPU 资源会更小些。\n\n从查询效率上看，使用 hash 字段方式的查询性能相对更稳定一些。因为 crc32 算出来的值虽然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近 1。而倒序存储方式毕竟还是用的前缀索引的方式，也就是说还是会增加扫描行数。\n\n\n\n\n字符串索引小结\n直接创建完整索引，这样可能比较占用空间；\n创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引；\n倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题；\n创建 hash 字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描。\n\n","slug":"事务隔离","date":"2022-08-31T12:22:15.000Z","categories_index":"数据库","tags_index":"MySQL,框架学习,基础概念","author_index":"依水何安"},{"id":"77b268d8a20b2f9b34be681cdfe52894","title":"MySQL框架&日志系统","content":"\n\n\n\n\n\n\n\n\n之前在字节青训营因为大作业的相关内容也接触过一些数据库方向的知识，但一直苦于没有系统性的学习和整理，所以这系列文章来总结和记录一些，个人觉得比较重要的内容，以便以后复习使用。主要参考文章是极客时间的MySQL45讲：MySQL 实战 45 讲 \nMySQL框架\n\n\n\n连接器\n\n\n\n\n\n\n\n\n连接器负责跟客户端建立链接、获取权限、维持和管理链接。常见的连接命令为：\n&gt;mysql -h$ip -P$port -u$user -p\n连接器在完成连接操作之后会进入Sleep状态表示空闲连接，而wait_timeout参数可以控制连接器自动断开的时间，一般默认为8小时。\n\n解决占用内存过多\n\n定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。\n如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。\n\n\n\n查询缓存\n\n\n\n\n\n\n\n\nMySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。\n查询缓存往往弊大于利，因为查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。\n分析器\n\n\n\n\n\n\n\n\n分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么。\nMySQL 从你输入的”select”这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名 T”，把字符串“ID”识别成“列 ID”。\n做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。\n优化器\n\n\n\n\n\n\n\n\n优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。\n执行器\n\n\n\n\n\n\n\n\nMySQL 通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。\n执行器的业务逻辑一般为：\n\n验证权限\n调用引擎接口执行逻辑\n重复逻辑知道表的最后一行\n返回满足条件的的行记录作为结果返回给客户端\n\n\n查询日志\n你会在数据库的慢查询日志中看到一个 rows_examined 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。\n在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 rows_examined 并不是完全相同的。\n\n\n日志系统mysql&gt; create table T(ID int primary key, c int);\n\n这是一条最常见的更新语句，例如我们现在要将ID&#x3D;2 这一行的值加 1，SQL 语句就会这么写：\nmysql&gt; update T set c&#x3D;c+1 where ID&#x3D;2;\n\n在上文中我们已经介绍了查询语句的流程,那么我们来整理一下更新语句的流程：\n\n连接连接器，验证权限\n分析器分析更新语句\n清空缓存结果\n优化器决定ID索引\n执行器执行更新\nredo log（重做日志）&amp; binlog（归档日志）\n\nredo log 重做日志\n\n\n\n\n\n\n\n\n在 MySQL 里也有这个问题，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。为了解决这个问题，MySQL 的设计者就用了redo log来提升更新效率。\nMySQL 里经常说到的 WAL 技术，WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘。具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。下图为 redo log的示意图。\n\n\n\n\nnnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB。从头开始写，写到末尾就又回到开头循环写。\n\nredo log示意图详解\nwrite pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。\nwrite pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。\n\n\nflush机制由于这个机制的存在，当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。\n一下这些情况就会发生数据库的flush（刷脏页，也就是将脏页恢复为一直内容）:\n\nInnoDB 的 redo log 写满了。这时候系统会停止所有更新操作，把 checkpoint 往前推进，redo log 留出空间可以继续写。\n系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。\nMySQL 认为系统“空闲”的时候。\nMySQL 正常关闭的情况。这时候，MySQL 会把内存的脏页都 flush 到磁盘上，这样下次 MySQL 启动的时候，就可以直接从磁盘上读数据，启动速度会很快。\n\n分析一下分析一下上面四种场景对性能的影响。\n其中，第三种情况是属于 MySQL 空闲时的操作，这时系统没什么压力，而第四种场景是数据库本来就要关闭了。这两种情况下，你不会太关注“性能”问题。\n第一种是“redo log 写满了，要 flush 脏页”，这种情况是 InnoDB 要尽量避免的。因为出现这种情况的时候，整个系统就不能再接受更新了，所有的更新都必须堵住。如果你从监控上看，这时候更新数会跌为 0。\n第二种是“内存不够用了，要先将脏页写到磁盘”，这种情况其实是常态。InnoDB 用缓冲池（buffer pool）管理内存，缓冲池中的内存页有三种状态：\n\n第一种是，还没有使用的；\n第二种是，使用了并且是干净页；\n第三种是，使用了并且是脏页。\n\n\n\n\n\n\n\n提示\nInnoDB 的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。\n而当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。这时候只能把最久不使用的数据页从内存中淘汰掉：如果要淘汰的是一个干净页，就直接释放出来复用；但如果是脏页呢，就必须将脏页先刷到磁盘，变成干净页后才能复用。\n\n\n刷脏页虽然是常态，但是出现以下这两种情况，都是会明显影响性能的：\n\n一个查询要淘汰的脏页个数太多，会导致查询的响应时间明显变长；\n日志写满，更新全部堵住，写性能跌为 0，这种情况对敏感业务来说，是不能接受的。\n\n所以，InnoDB 需要有控制脏页比例的机制，来尽量避免上面的这两种情况。\nInnoDB 刷脏页的控制策略首先，你要正确地告诉 InnoDB 所在主机的 IO 能力，这样 InnoDB 才能知道需要全力刷脏页的时候，可以刷多快。\n这就要用到 innodb_io_capacity 这个参数了，它会告诉 InnoDB 你的磁盘能力。这个值我建议你设置成磁盘的 IOPS。磁盘的 IOPS 可以通过 fio 这个工具来测试，下面的语句是我用来测试磁盘随机读写的命令：\nfio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest \n\n其实，因为没能正确地设置 innodb_io_capacity 参数，而导致的性能问题也比比皆是。\nInnoDB 的刷盘速度就是要参考这两个因素：\n\n一个是脏页比例。参数 innodb_max_dirty_pages_pct 是脏页比例上限，默认值是 75%。InnoDB 会根据当前的脏页比例（假设为 M），算出一个范围在 0 到 100 之间的数字。InnoDB 每次写入的日志都有一个序号，当前写入的序号跟 checkpoint 对应的序号之间的差值，我们假设为 N。InnoDB 会根据这个 N 算出一个范围在 0 到 100 之间的数字，这个计算公式可以记为 F2(N)。根据上述算得的 F1(M) 和 F2(N) 两个值，取其中较大的值记为 R，之后引擎就可以按照 innodb_io_capacity 定义的能力乘以 R% 来控制刷脏页的速度。如图所示：\n\n\n\n\n\n\n\n一个是 redo log 写盘速度。\n\n\n\n\n\n\n\n\n注意\n一旦一个查询请求需要在执行过程中先 flush 掉一个脏页时，这个查询就可能要比平时慢了。而 MySQL 中的一个机制，可能让你的查询会更慢：在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉；而且这个把“邻居”拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷。\n在 InnoDB 中，innodb_flush_neighbors 参数就是用来控制这个行为的，值为 1 的时候会有上述的“连坐”机制，值为 0 时表示不找邻居，自己刷自己的。\n找“邻居”这个优化在机械硬盘时代是很有意义的，可以减少很多随机 IO。机械硬盘的随机 IOPS 一般只有几百，相同的逻辑操作减少随机 IO 就意味着系统性能的大幅度提升。\n而如果使用的是 SSD 这类 IOPS 比较高的设备的话，我就建议你把 innodb_flush_neighbors 的值设置成 0。因为这时候 IOPS 往往不是瓶颈，而“只刷自己”，就能更快地执行完必要的刷脏页操作，减少 SQL 语句响应时间。\n在 MySQL 8.0 中，innodb_flush_neighbors 参数的默认值已经是 0 了。\n\n\ncrash-safe\n\n\n\n\n\n\n\n\n有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。\nbinlog（归档日志）\n\n\n\n\n\n\n\n\n实际上，在框架中我们不难发现MySQL有一块是 Server 层，它主要做的是 MySQL 功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。如果说redo log是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，称为 binlog（归档日志）。\n仅仅依靠binlog只能完成归档服务而不能实现crash-safe 能力，因此 MySQL为了实现存储引擎的crash-safe 能力带来了另一套日志系统也就是redo log。这两种日志主要有以下三点不同：\n\nredo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。\nredo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID&#x3D;2 这一行的 c 字段加 1 ”。\nredo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。\n\n此时我们就可以总结执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程：\n\n执行器先找引擎取 ID&#x3D;2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID&#x3D;2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。\n执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。\n引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。\n执行器生成这个操作的 binlog，并把 binlog 写入磁盘。\n执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。\n\n两阶段提交\n\n\n\n\n\n\n\n\n仍然用前面的 update 语句来做例子。假设当前 ID&#x3D;2 的行，字段 c 的值是 0，再假设执行 update 语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了 crash，会出现什么情况呢？\n\n先写 redo log 后写 binlog。假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。\n先写 binlog 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。\n\n虽然表面上看起来，发生宕机的概率不高，但是在需要扩容的时候，也就是需要再多搭建一些备库来增加系统的读能力的时候，现在常见的做法也是用全量备份加上应用 binlog 来实现的，这个“不一致”就会导致你的线上出现主从数据库不一致的情况。\nredo log 用于保证 crash-safe 能力。innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。\nsync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。只有持久化的存储才能保证数据一场重启之后不会丢失。\n\n\n\n\n\n\nQuestion\n定期全量备份的周期“取决于系统重要性，有的是一天一备，有的是一周一备”。那么在什么场景下，一天一备会比一周一备更有优势呢？或者说，它影响了这个数据库系统的哪个指标？\n\n\n\nAnswer\n该问题主要取决于binlog的规模，一天一备的binlog规模较小，那么恢复到误删时刻的时间和成本自然也就越短，而一周一备虽然恢复时间较长，但是如果发现问题时距离误操作已经过去了好几天，那么也仍然可以恢复，也就是后悔的容错时间更长。因此可以一天一备或者短周期备份后，再额外全库备份一次，以防止意外的发生也比较兼顾。\n\n\n异常重启\n\n\n\n如果在图中时刻 A 的地方，也就是写入 redo log 处于 prepare 阶段之后、写 binlog 之前，发生了崩溃（crash），由于此时 binlog 还没写，redo log 也还没提交，所以崩溃恢复的时候，这个事务会回滚。这时候，binlog 还没写，所以也不会传到备库。到这里，大家都可以理解。\n大家出现问题的地方，主要集中在时刻 B，也就是 binlog 写完，redo log 还没 commit 前发生 crash，那崩溃恢复的时候 MySQL 会怎么处理？\nMySQL崩溃恢复时的判断规则。\n\n如果 redo log 里面的事务是完整的，也就是已经有了 commit 标识，则直接提交；\n如果 redo log 里面的事务只有完整的 prepare，则判断对应的事务 binlog 是否存在并完整：a. 如果是，则提交事务；b. 否则，回滚事务。\n\n这里，时刻 B 发生 crash 对应的就是 2(a) 的情况，崩溃恢复过程中事务会被提交。\n\n\n\n\n\n\nQuestion\nMySQL 怎么知道 binlog 是完整的?\n\n\n一个事务的 binlog 是有完整格式的：\n\nstatement 格式的 binlog，最后会有 COMMIT；\nrow 格式的 binlog，最后会有一个 XID event。\n\n另外，在 MySQL 5.6.2 版本以后，还引入了 binlog-checksum 参数，用来验证 binlog 内容的正确性。对于 binlog 日志由于磁盘原因，可能会在日志中间出错的情况，MySQL 可以通过校验 checksum 的结果来发现。所以，MySQL 还是有办法验证事务 binlog 的完整性的。\n\n\n\n\n\n\nQuestion\nredo log 和 binlog 是怎么关联起来的?\n\n\n它们有一个共同的数据字段，叫 XID。崩溃恢复的时候，会按顺序扫描 redo log：\n\n如果碰到既有 prepare、又有 commit 的 redo log，就直接提交；\n如果碰到只有 parepare、而没有 commit 的 redo log，就拿着 XID 去 binlog 找对应的事务。\n\n\n\n\n\n\n\nQuestion\n处于 prepare 阶段的 redo log 加上完整 binlog，重启就能恢复，MySQL 为什么要这么设计?\n\n\n在时刻 B，也就是 binlog 写完以后 MySQL 发生崩溃，这时候 binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。\n所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。\n\n\n\n\n\n\nQuestion\n不引入两个日志，也就没有两阶段提交的必要了。只用 binlog 来支持崩溃恢复，又能支持归档，不就可以了？\n\n\nmysql 为什么不能用binlog来做数据恢复？_JackMa_的博客-CSDN博客_binlog为什么不支持崩溃恢复\n\n\n\n\n\n\nQuestion\n能不能反过来，只用 redo log，不要 binlog\n\n\n如果只从崩溃恢复的角度来讲是可以的。你可以把 binlog 关掉，这样就没有两阶段提交了，但系统依然是 crash-safe 的。\n但是，如果你了解一下业界各个公司的使用场景的话，就会发现在正式的生产库上，binlog 都是开着的。因为 binlog 有着 redo log 无法替代的功能。\n一个是归档。redo log 是循环写，写到末尾是要回到开头继续写的。这样历史日志没法保留，redo log 也就起不到归档的作用。\n一个就是 MySQL 系统依赖于 binlog。binlog 作为 MySQL 一开始就有的功能，被用在了很多地方。其中，MySQL 系统高可用的基础，就是 binlog 复制。\n还有很多公司有异构系统（比如一些数据分析系统），这些系统就靠消费 MySQL 的 binlog 来更新自己的数据。关掉 binlog 的话，这些下游系统就没法输入了。\n总之，由于现在包括 MySQL 高可用在内的很多系统机制都依赖于 binlog，所以“鸠占鹊巢”redo log 还做不到。你看，发展生态是多么重要。\n\n\n\n\n\n\nQuestion\nredo log buffer 是什么？是先修改内存，还是先写 redo log 文件？\n\n\n在一个事务的更新过程中，日志是要写多次的。比如下面这个事务：\nbegin;\ninsert into t1 ...\ninsert into t2 ...\ncommit;\n\n这个事务要往两个表中插入记录，插入数据的过程中，生成的日志都得先保存起来，但又不能在还没 commit 的时候就直接写到 redo log 文件里。\n所以，redo log buffer 就是一块内存，用来先存 redo 日志的。也就是说，在执行第一个 insert 的时候，数据的内存被修改了，redo log buffer 也写入了日志。\n但是，真正把日志写到 redo log 文件（文件名是 ib_logfile+ 数字），是在执行 commit 语句的时候做的。\n","slug":"MySQL框架","date":"2022-08-29T10:22:15.000Z","categories_index":"数据库","tags_index":"MySQL,框架学习,基础概念","author_index":"依水何安"},{"id":"298ae6749824eedc891feac490554768","title":"MySQL幻读","content":"\n\n\n\n\n\n\n\n\n幻读指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。\n\n\n\n\n\n\n提示\n这里对“幻读”做一个说明：\n\n在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此，幻读在“当前读”下才会出现。\n上面 session B 的修改结果，被 session A 之后的 select 语句用“当前读”看到，不能称为幻读。幻读仅专指“新插入的行”。\n\n\n\n幻读存在的问题\n首先是语义上的。导致幻读的操作会破坏第一次当前读的语义。\n\n其次，是数据一致性的问题。锁的设计是为了保证数据的一致性。而这个一致性，不止是数据库内部数据状态在此刻的一致性，还包含了数据和日志在逻辑上的一致性。\n\n暴力加锁无法解决问题。即使把所有的记录都加上锁，还是阻止不了新插入的记录。\n\n\n幻读的解决方法","slug":"MySQL幻读","date":"2022-09-25T15:36:04.000Z","categories_index":"数据库","tags_index":"MySQL,框架学习,基础概念","author_index":"依水何安"},{"id":"dd59c704134a0428540392e3edcac680","title":"MySQL功能机制","content":"数据库表的空间回收机制\n\n\n\n\n\n\n\n\n一个 InnoDB 表包含两部分，即：表结构定义和数据。因为表结构定义占用的空间很小，所以我们今天主要讨论的是表数据。\n参数 innodb_file_per_table表数据既可以存在共享表空间里，也可以是单独的文件。这个行为是由参数 innodb_file_per_table 控制的：\n\n这个参数设置为 OFF 表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起；\n这个参数设置为 ON 表示的是，每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中。\n\n\n\n\n\n\n\n提示\n将 innodb_file_per_table 设置为 ON，是推荐做法\n\n\n数据删除流程\n\n\n\n假设，我们要删掉 R4 这个记录，InnoDB 引擎只会把 R4 这个记录标记为删除。如果之后要再插入一个 ID 在 300 和 600 之间的记录时，可能会复用这个位置。但是，磁盘文件的大小并不会缩小。\nInnoDB 的数据是按页存储的,所以当删除整个页的时候，整个数据也都可以被复用，但是，数据页的复用跟记录的复用是不同的。\n\n记录的复用，只限于符合范围条件的数据。比如上面的这个例子，R4 这条记录被删除后，如果插入一个 ID 是 400 的行，可以直接复用这个空间。但如果插入的是一个 ID 是 800 的行，就不能复用这个位置了。\n而当整个页从 B+ 树里面摘掉以后，可以复用到任何位置。如果相邻的两个数据页利用率都很小，系统就会把这两个页上的数据合到其中一个页上，另外一个数据页就被标记为可复用。\n\ndelete 命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。也就是说，通过 delete 命令是不能回收表空间的。这些可以复用，而没有被使用的空间，看起来就像是“空洞”。\n不止是删除数据会造成空洞，插入数据也会。\n如果数据是按照索引递增顺序插入的，那么索引是紧凑的。但如果数据是随机插入的，就可能造成索引的数据页分裂。\n也就是说，经过大量增删改的表，都是可能是存在空洞的。所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的。\n因此我们就需要了解重建表。\n重建表\n表 A需要怎么做空间收缩\n你可以新建一个与表 A 结构相同的表 B，然后按照主键 ID 递增的顺序，把数据一行一行地从表 A 里读出来再插入到表 B 中。\n由于表 B 是新建的表，所以表 A 主键索引上的空洞，在表 B 中就都不存在了。显然地，表 B 的主键索引更紧凑，数据页的利用率也更高。如果我们把表 B 作为临时表，数据从表 A 导入表 B 的操作完成后，用表 B 替换 A，从效果上看，就起到了收缩表 A 空间的作用。\n\n\n\n\n\n\n\n\n\n注意\n在 MySQL 5.5 版本之前，这个命令的执行流程跟我们前面描述的差不多，区别只是这个临时表 B 不需要你自己创建，MySQL 会自动完成转存数据、交换表名、删除旧表的操作。\n\n\n而在MySQL 5.6 版本开始引入的 Online DDL，对这个操作流程做了优化。\n\n建立一个临时文件，扫描表 A 主键的所有数据页；\n用数据页中表 A 的记录生成 B+ 树，存储到临时文件中；\n生成临时文件的过程中，将所有对 A 的操作记录在一个日志文件（row log）中，对应的是图中 state2 的状态；\n临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表 A 相同的数据文件，对应的就是图中 state3 的状态；\n用临时文件替换表 A 的数据文件。\n\n由于日志文件记录和重放操作这个功能的存在，这个方案在重建表的过程中，允许对表 A 做增删改操作。这也就是 Online DDL 名字的来源。\nOnline 和 inplace说到 Online，我还要再和你澄清一下它和另一个跟 DDL 有关的、容易混淆的概念 inplace 的区别。\n你可能注意到了，在 MySQL 5.5 版本之前中，我们把表 A 中的数据导出来的存放位置叫作 tmp_table。这是一个临时表，是在 server 层创建的。\n在MySQL 5.6 版本开始，根据表 A 重建出来的数据是放在“tmp_file”里的，这个临时文件是 InnoDB 在内部创建出来的。整个 DDL 过程都在 InnoDB 内部完成。对于 server 层来说，没有把数据挪动到临时表，是一个“原地”操作，这就是“inplace”名称的来源。\n所以，我现在问你，如果你有一个 1TB 的表，现在磁盘间是 1.2TB，能不能做一个 inplace 的 DDL 呢？\n答案是不能。因为，tmp_file 也是要占用临时空间的。\n如果说这两个逻辑之间的关系是什么的话，可以概括为：\n\nDDL 过程如果是 Online 的，就一定是 inplace 的；\n反过来未必，也就是说 inplace 的 DDL，有可能不是 Online 的。截止到 MySQL 8.0，添加全文索引（FULLTEXT index）和空间索引 (SPATIAL index) 就属于这种情况。\n\n\n\n\n\n\n\n提示\noptimize table、analyze table 和 alter table 这三种方式重建表的区别。\n\n从 MySQL 5.6 版本开始，alter table t engine &#x3D; InnoDB（也就是 recreate）默认的就是上面 Online DDL的流程了；\nanalyze table t 其实不是重建表，只是对表的索引信息做重新统计，没有修改数据，这个过程中加了 MDL 读锁；\noptimize table t 等于 recreate+analyze。\n\n\n\ncount(*) 的实现方式在不同的 MySQL 引擎中，count(*) 有不同的实现方式。\n\nMyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个数，效率很高；\n而 InnoDB 引擎就麻烦了，它执行 count(*) 的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。\n\n\n\n\n\n\n\n提示\n在这篇文章里讨论的是没有过滤条件的 count(*)，如果加了 where 条件的话，MyISAM 表也是不能返回得这么快的。\n\n\n为什么 InnoDB 不跟 MyISAM 一样，也把数字存起来呢？\n这是因为即使是在同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB 表“应该返回多少行”也是不确定的。这里，我用一个算 count(*) 的例子来为你解释一下。\n假设表 t 中现在有 10000 条记录，我们设计了三个用户并行的会话。\n\n会话 A 先启动事务并查询一次表的总行数；\n会话 B 启动事务，插入一行后记录后，查询表的总行数；\n会话 C 先启动一个单独的语句，插入一行记录后，查询表的总行数。\n\n\n\n\n\n三个会话 A、B、C 会同时查询表 t 的总行数，但拿到的结果却不同。\n\n\n\n\n\n\ncount(*) 优化\nInnoDB 是索引组织表，主键索引树的叶子节点是数据，而普通索引树的叶子节点是主键值。所以，普通索引树比主键索引树小很多。对于 count(*) 这样的操作，遍历哪个索引树得到的结果逻辑上都是一样的。因此，MySQL 优化器会找到最小的那棵树来遍历。 在保证逻辑正确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一。\n\n\n如果你用过 show table status 命令的话，就会发现这个命令的输出结果里面也有一个 TABLE_ROWS 用于显示这个表当前有多少行，这个命令执行挺快的，那这个 TABLE_ROWS 能代替 count(*) 吗？\n\n\n\n\n\n\n\n特别注意\n实际上，TABLE_ROWS 就是从这个采样估算得来的，因此它也很不准。有多不准呢，官方文档说误差可能达到 40% 到 50%。所以，show table status 命令显示的行数也不能直接使用。\n\n\n总的来说就是:\n\nMyISAM 表虽然 count(*) 很快，但是不支持事务；\nshow table status 命令虽然返回很快，但是不准确；\nInnoDB 表直接 count(*) 会遍历全表，虽然结果准确，但会导致性能问题。\n\n数据行数的存储方式因此如果想要计数我们需要找个地方存储行数：\n1.用缓存系统保存计数你可以用一个 Redis 服务来保存这个表的总行数。这个表每被插入一行 Redis 计数就加 1，每被删除一行 Redis 计数就减 1。这种方式下，读和更新操作都很快，但你再想一下这种方式存在什么问题吗？\n没错，缓存系统可能会丢失更新。\nRedis 的数据不能永久地留在内存里，所以你会找一个地方把这个值定期地持久化存储起来。但即使这样，仍然可能丢失更新。试想如果刚刚在数据表中插入了一行，Redis 中保存的值也加了 1，然后 Redis 异常重启了，重启后你要从存储 redis 数据的地方把这个值读回来，而刚刚加 1 的这个计数操作却丢失了。\n当然了，这还是有解的。比如，Redis 异常重启以后，到数据库里面单独执行一次 count(*) 获取真实的行数，再把这个值写回到 Redis 里就可以了。异常重启毕竟不是经常出现的情况，这一次全表扫描的成本，还是可以接受的。\n但实际上，将计数保存在缓存系统中的方式，还不只是丢失更新的问题。即使 Redis 正常工作，这个值还是逻辑上不精确的：\n\n一种是，查到的 100 行结果里面有最新插入记录，而 Redis 的计数里还没加 1；\n另一种是，查到的 100 行结果里没有最新插入的记录，而 Redis 的计数里已经加了 1。\n\n2.在数据库保存计数用缓存系统保存计数有丢失数据和计数不精确的问题。那么，如果我们把这个计数直接放到数据库里单独的一张计数表 C 中，又会怎么样呢？\n首先，这解决了崩溃丢失的问题，InnoDB 是支持崩溃恢复不丢数据的。\n由于 InnoDB 要支持事务，从而导致 InnoDB 表不能把 count(*) 直接存起来，然后查询的时候直接返回形成的。\n\n\n\n\n虽然会话 B 的读操作仍然是在 T3 执行的，但是因为这时候更新事务还没有提交，所以计数值加 1 这个操作对会话 B 还不可见。\n因此，会话 B 看到的结果里， 查计数值和“最近 100 条记录”看到的结果，逻辑上就是一致的。\n不同count的用法\n\n\n\n\n\n\n\n\ncount() 是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加 1，否则不加。最后返回累计值。\n所以，count(*)、count(主键 id) 和 count(1) 都表示返回满足条件的结果集的总行数；而 count(字段），则表示返回满足条件的数据行里面，参数“字段”不为 NULL 的总个数。\n至于分析性能差别的时候，有几个原则：\n\nserver 层要什么就给什么；\nInnoDB 只给必要的值；\n现在的优化器只优化了 count(*) 的语义为“取行数”，其他“显而易见”的优化并没有做。\n\n对于 count(主键 id) 来说，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。\n对于 count(1) 来说，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。\n单看这两个用法的差别的话，你能对比出来，count(1) 执行得要比 count(主键 id) 快。因为从引擎返回 id 会涉及到解析数据行，以及拷贝字段值的操作。\n对于 count(字段) 来说：\n\n如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加；\n如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。\n\n也就是前面的第一条原则，server 层要什么字段，InnoDB 就返回什么字段。\n但是 count(*) 是例外，并不会把全部字段取出来，而是专门做了优化，不取值。count(*) 肯定不是 null，按行累加。\n\n强烈推荐这篇参考文章\nhttps://learnsql.com/blog/difference-between-count-distinct/ (这篇文章讲的非常好)\n\n\n排序机制全字段排序通常情况下，这个语句执行流程如下所示 ：\n\n初始化 sort_buffer，确定放入 name、city、age 这三个字段；\n从索引 city 找到第一个满足 city&#x3D;’杭州’条件的主键 id，也就是图中的 ID_X；\n到主键 id 索引取出整行，取 name、city、age 三个字段的值，存入 sort_buffer 中；\n从索引 city 取下一个记录的主键 id；\n重复步骤 3、4 直到 city 的值不满足查询条件为止，对应的主键 id 也就是图中的 ID_Y；\n对 sort_buffer 中的数据按照字段 name 做快速排序；\n按照排序结果取前 1000 行返回给客户端。\n\n\n\n\n\n\n\n\n注意\n“按 name 排序”这个动作，可能在内存中完成，也可能需要使用外部排序，这取决于排序所需的内存和参数 sort_buffer_size。\n\n\nrowid 排序在上面这个算法过程里面，只对原表的数据读了一遍，剩下的操作都是在 sort_buffer 和临时文件中执行的。但这个算法有一个问题，就是如果查询要返回的字段很多的话，那么 sort_buffer 里面要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差。\nmax_length_for_sort_data，是 MySQL 中专门控制用于排序的行数据的长度的一个参数。它的意思是，如果单行的长度超过这个值，MySQL 就认为单行太大，要换一个算法。\ncity、name、age 这三个字段的定义总长度是 36，我把 max_length_for_sort_data 设置为 16，我们再来看看计算过程有什么改变。\n新的算法放入 sort_buffer 的字段，只有要排序的列（即 name 字段）和主键 id。\n整个执行流程就变成如下所示的样子：\n\n初始化 sort_buffer，确定放入两个字段，即 name 和 id；\n从索引 city 找到第一个满足 city&#x3D;’杭州’条件的主键 id，也就是图中的 ID_X；\n到主键 id 索引取出整行，取 name、id 这两个字段，存入 sort_buffer 中；\n从索引 city 取下一个记录的主键 id；\n重复步骤 3、4 直到不满足 city&#x3D;’杭州’条件为止，也就是图中的 ID_Y；\n对 sort_buffer 中的数据按照字段 name 进行排序；\n遍历排序结果，取前 1000 行，并按照 id 的值回到原表中取出 city、name 和 age 三个字段返回给客户端。\n\n全字段排序 VS rowid 排序如果 MySQL 实在是担心排序内存太小，会影响排序效率，才会采用 rowid 排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。\n如果 MySQL 认为内存足够大，会优先选择全字段排序，把需要的字段都放到 sort_buffer 中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。\n这也就体现了 MySQL 的一个设计思想：如果内存够，就要多利用内存，尽量减少磁盘访问。\n索引优化排序性能联合索引其实，并不是所有的 order by 语句，都需要排序操作的。从上面分析的执行过程，我们可以看到，MySQL 之所以需要生成临时表，并且在临时表上做排序操作，其原因是原来的数据都是无序的。\n我们可以在这个市民表上创建一个 city 和 name 的联合索引，对应的 SQL 语句是：\nalter table t add index city_user(city, name);\n\n在这个索引里面，我们依然可以用树搜索的方式定位到第一个满足 city&#x3D;’杭州’的记录，并且额外确保了，接下来按顺序取“下一条记录”的遍历过程中，只要 city 的值是杭州，name 的值就一定是有序的。\n这样整个查询过程的流程就变成了：\n\n从索引 (city,name) 找到第一个满足 city&#x3D;’杭州’条件的主键 id；\n到主键 id 索引取出整行，取 name、city、age 三个字段的值，作为结果集的一部分直接返回；\n从索引 (city,name) 取下一个记录主键 id；\n重复步骤 2、3，直到查到第 1000 条记录，或者是不满足 city&#x3D;’杭州’条件时循环结束。\n\n覆盖索引覆盖索引是指，索引上的信息足够满足查询请求，不需要再回到主键索引上去取数据。\n按照覆盖索引的概念，我们可以再优化一下这个查询语句的执行流程。\n针对这个查询，我们可以创建一个 city、name 和 age 的联合索引，对应的 SQL 语句就是：\nalter table t add index city_user_age(city, name, age);\n\n这时，对于 city 字段的值相同的行来说，还是按照 name 字段的值递增排序的，此时的查询语句也就不再需要排序了。这样整个查询语句的执行流程就变成了：\n\n从索引 (city,name,age) 找到第一个满足 city&#x3D;’杭州’条件的记录，取出其中的 city、name 和 age 这三个字段的值，作为结果集的一部分直接返回；\n从索引 (city,name,age) 取下一个记录，同样取出这三个字段的值，作为结果集的一部分直接返回；\n重复执行步骤 2，直到查到第 1000 条记录，或者是不满足 city&#x3D;’杭州’条件时循环结束。\n\n随机消息\n\n\n\n\n\n\n\n\n对于 InnoDB 表来说，执行全字段排序会减少磁盘访问，因此会被优先选择。\n对于内存表，回表过程只是简单地根据数据行的位置，直接访问内存得到数据，根本不会导致多访问磁盘。优化器没有了这一层顾虑，那么它会优先考虑的，就是用于排序的行越小越好了，所以，MySQL 这时就会选择 rowid 排序。\n这条语句的执行流程是这样的：\n\n创建一个临时表。这个临时表使用的是 memory 引擎，表里有两个字段，第一个字段是 double 类型，为了后面描述方便，记为字段 R，第二个字段是 varchar(64) 类型，记为字段 W。并且，这个表没有建索引。\n从 words 表中，按主键顺序取出所有的 word 值。对于每一个 word 值，调用 rand() 函数生成一个大于 0 小于 1 的随机小数，并把这个随机小数和 word 分别存入临时表的 R 和 W 字段中，到此，扫描行数是 10000。\n现在临时表有 10000 行数据了，接下来你要在这个没有索引的内存临时表上，按照字段 R 排序。\n初始化 sort_buffer。sort_buffer 中有两个字段，一个是 double 类型，另一个是整型。\n从内存临时表中一行一行地取出 R 值和位置信息（我后面会和你解释这里为什么是“位置信息”），分别存入 sort_buffer 中的两个字段里。这个过程要对内存临时表做全表扫描，此时扫描行数增加 10000，变成了 20000。\n在 sort_buffer 中根据 R 的值进行排序。注意，这个过程没有涉及到表操作，所以不会增加扫描行数。\n排序完成后，取出前三个结果的位置信息，依次到内存临时表中取出 word 值，返回给客户端。这个过程中，访问了表的三行数据，总扫描行数变成了 20003。\n\n磁盘临时表\n\n\n\n\n\n\n\n\ntmp_table_size 这个配置限制了内存临时表的大小，默认值是 16M。如果临时表大小超过了 tmp_table_size，那么内存临时表就会转成磁盘临时表。\n磁盘临时表使用的引擎默认是 InnoDB，是由参数 internal_tmp_disk_storage_engine 控制的。\n当使用磁盘临时表的时候，对应的就是一个没有显式索引的 InnoDB 表的排序过程。\n采用是 MySQL 5.6 版本引入的一个新的排序算法，即：优先队列排序算法。接下来，我们就看看为什么没有使用临时文件的算法，也就是归并排序算法，而是采用了优先队列排序算法。\n其实，我们现在的 SQL 语句，只需要取 R 值最小的 3 个 rowid。但是，如果使用归并排序算法的话，虽然最终也能得到前 3 个值，但是这个算法结束后，已经将 10000 行数据都排好序了。\n也就是说，后面的 9997 行也是有序的了。但，我们的查询并不需要这些数据是有序的。所以，想一下就明白了，这浪费了非常多的计算量。\n而优先队列算法，就可以精确地只得到三个最小值，执行流程如下：\n\n对于这 10000 个准备排序的 (R,rowid)，先取前三行，构造成一个堆；\n\n（对数据结构印象模糊的同学，可以先设想成这是一个由三个元素组成的数组）\n\n取下一个行 (R’,rowid’)，跟当前堆里面最大的 R 比较，如果 R’小于 R，把这个 (R,rowid) 从堆中去掉，换成 (R’,rowid’)；\n重复第 2 步，直到第 10000 个 (R’,rowid’) 完成比较。\n\n随机排序方法我们先把问题简化一下，如果只随机选择 1 个 word 值，可以怎么做呢？思路上是这样的：\n\n取得这个表的主键 id 的最大值 M 和最小值 N;\n用随机函数生成一个最大值到最小值之间的数 X &#x3D; (M-N)*rand() + N;\n取不小于 X 的第一个 ID 的行。\n\n这个方法效率很高，因为取 max(id) 和 min(id) 都是不需要扫描索引的，而第三步的 select 也可以用索引快速定位，可以认为就只扫描了 3 行。但实际上，这个算法本身并不严格满足题目的随机要求，因为 ID 中间可能有空洞，因此选择不同行的概率不一样，不是真正的随机。\n所以，为了得到严格随机的结果，你可以用下面这个流程:\n\n取得整个表的行数，并记为 C。\n取得 Y &#x3D; floor(C * rand())。 floor 函数在这里的作用，就是取整数部分。\n再用 limit Y,1 取得一行。\n\n现在，我们再看看，如果我们按照随机算法 2 的思路，要随机取 3 个 word 值呢？你可以这么做：\n\n取得整个表的行数，记为 C；\n根据相同的随机方法得到 Y1、Y2、Y3；\n再执行三个 limit Y, 1 语句得到三行数据。\n\n","slug":"MySQL功能机制","date":"2022-09-22T15:12:15.000Z","categories_index":"数据库","tags_index":"MySQL,框架学习,基础概念","author_index":"依水何安"},{"id":"88440a5b541fa0a94c29d4531936445a","title":"MySQL锁设计","content":"\n\n\n\n\n\n\n\n\n根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类。\n全局锁顾名思义，全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。\n当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：\n\n数据更新语句（数据的增删改）\n数据定义语句（包括建表、修改表结构等）\n更新类事务的提交语句。\n\n全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都 select 出来存成文本。\n\n\n\n\n\n\n\n特别注意\n但是让整库都只读，听上去就很危险：\n\n如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆；\n如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从延迟。\n\n\n\n在可重复读隔离级别下开启一个事务。也可以解决这个问题。\n官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数–single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。\n为什么还需要 FTWRL 呢？一致性读是好，但前提是引擎要支持这个隔离级别。比如，对于 MyISAM 这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用 FTWRL 命令了。\n所以，single-transaction 方法只适用于所有的表使用事务引擎的库。如果有的表使用了不支持事务的引擎，那么备份就只能通过 FTWRL 方法。这往往是 DBA 要求业务开发人员使用 InnoDB 替代 MyISAM 的原因之一。\n既然要全库只读，为什么不使用 set global readonly&#x3D;true 的方式呢？确实 readonly 方式也可以让全库进入只读状态，但我还是会建议你用 FTWRL 方式，主要有两个原因：\n\n一是，在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，我不建议你使用。\n二是，在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。\n\n表级锁\n\n\n\n\n\n\n\n\nMySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。\n表锁的语法是 lock tables … read&#x2F;write。与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。\n举个例子, 如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表。\n而对于 InnoDB 这种支持行锁的引擎，一般不使用 lock tables 命令来控制并发，毕竟锁住整个表的影响面还是太大。\n另一类表级的锁是 MDL（metadata lock)。MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。\n因此，在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。\n\n读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。\n读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。\n\n行锁\n\n\n\n\n\n\n\n\nMySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。\n在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。\n知道了这个设定，对我们使用事务有什么帮助呢？那就是，如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。\n死锁和死锁检测当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。\n\n\n\n\n这时候，事务 A 在等待事务 B 释放 id&#x3D;2 的行锁，而事务 B 在等待事务 A 释放 id&#x3D;1 的行锁。 事务 A 和事务 B 在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有两种策略：\n\n一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。\n另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。\n\n\n\n\n\n\n\n提示\n在 InnoDB 中，innodb_lock_wait_timeout 的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。\n但是，我们又不可能直接把这个时间设置成一个很小的值，比如 1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。\n\n\n所以，正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且 innodb_deadlock_detect 的默认值本身就是 on。主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。\n每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n) 的操作。假设有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的 CPU 资源。因此，你就会看到 CPU 利用率很高，但是每秒却执行不了几个事务。\n怎么解决由这种热点行更新导致的性能问题呢？\n\n一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的。\n\n另一个思路是控制并发度。根据上面的分析，你会发现如果并发能够控制住，比如同一行同时最多只有 10 个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。一个直接的想法就是，在客户端做并发控制。但是，你会很快发现这个方法不太可行，因为客户端很多。我见过一个应用，有 600 个客户端，这样即使每个客户端控制到只有 5 个并发线程，汇总到数据库服务端以后，峰值并发数也可能要达到 3000。\n\n\n\n如果团队里暂时没有数据库方面的专家，不能实现这样的方案，能不能从设计上优化这个问题呢？\n你可以考虑通过将一行改成逻辑上的多行来减少锁冲突。还是以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1&#x2F;10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。\n这个方案看上去是无损的，但其实这类方案需要根据业务逻辑做详细设计。如果账户余额可能会减少，比如退票逻辑，那么这时候就需要考虑当一部分行记录变成 0 的时候，代码要有特殊处理。\n\n\n“快照”在 MVCC 里是怎么工作的？\n\n\n\n\n\n\n\n\n在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。\nInnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。\n而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。\n也就是说，数据表中的一行记录，其实可能有多个版本 (row)，每个版本有自己的 row trx_id。\n\n\n\n\n\n\n提示\n更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。\n\n\n而读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是：\n\n在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；\n在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。\n\nMySQL实战45讲_8_从一个问题来加深对 mysql\n","slug":"全局锁","date":"2022-09-17T12:22:15.000Z","categories_index":"数据库","tags_index":"MySQL,框架学习,基础概念","author_index":"依水何安"}]